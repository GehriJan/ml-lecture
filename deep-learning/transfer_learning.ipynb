{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "### Aufgabe\n",
    "_Wählen Sie nun 5 andere Hunderassen aus. Nutzen Sie Transfer Learning zur Klassifikation dieser\n",
    "Rassen, d.h. laden Sie die Gewichte aus Ihrem in Teilaufgabe B5 trainierten Modell als initiale\n",
    "Gewichte für das neue Training. Beschreiben Sie ihr Vorgehen im Kurzreport und setzen Sie die\n",
    "Evaluation dieses erneuten Trainings in Relation zu den Evaluationsergebnissen ihres ursprünglichen\n",
    "Modells._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "notebook_dir = Path().resolve()\n",
    "sys.path.append(str(notebook_dir.parent))\n",
    "\n",
    "from utils.setup import setup_dataset, generateDatasetArrays\n",
    "\n",
    "# load configuration\n",
    "import config as Config\n",
    "\n",
    "# config overrides\n",
    "Config.DOG_LABEL_IDS = [5, 12, 45, 86, 100] # 5 andere Hunderassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, label_lookup_table, info = setup_dataset('../dataset', labels=Config.DOG_LABEL_IDS)\n",
    "\n",
    "def preprocess(data):\n",
    "    img = data[\"image\"]\n",
    "    data[\"original_image\"] = tf.identity(data[\"image\"])\n",
    "    img = tf.image.resize(img, Config.RESIZE_SIZE)\n",
    "    data[\"image\"] = img\n",
    "    data[\"label\"] = label_lookup_table.lookup(data[\"label\"])\n",
    "    return data\n",
    "\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "        .map(preprocess)\n",
    "        .shuffle(1000)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "        .map(preprocess)\n",
    "        .shuffle(100)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 22:19:27.829133: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117964800 exceeds 10% of free system memory.\n",
      "2024-12-13 22:19:27.915383: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117964800 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(os.path.join(Config.MODEL_FOLDER, Config.MODEL_FILENAME))\n",
    "\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(Config.NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "# Freeze the layers except the last three\n",
    "for layer in model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_history(history):\n",
    "\n",
    "    # Loss-Werte\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    # Accuracy-Werte (falls verwendet)\n",
    "    accuracy = history.get('accuracy')\n",
    "    val_accuracy = history.get('val_accuracy')\n",
    "\n",
    "    # Epochen erstellen\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # Plot für den Loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, 'bo-', label='Trainings Loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label='Validierungs Loss')\n",
    "    plt.title('Trainings und Validierungs Loss')\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot für die Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, 'bo-', label='Trainings Genauigkeit')\n",
    "    plt.plot(epochs, val_accuracy, 'ro-', label='Validierungs Genauigkeit')\n",
    "    plt.title('Trainings und Validierungs Genauigkeit')\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('Genauigkeit')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 22:19:28.128582: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2024-12-13 22:19:28.389189: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images_combined)\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels_combined)\n\u001b[0;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnew_model\u001b[49m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      9\u001b[0m     X_train_new, y_train_new,\n\u001b[1;32m     10\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test_new, y_test_new),\n\u001b[1;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Feinabstimmung des gesamten Modells\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m new_model\u001b[38;5;241m.\u001b[39mlayers:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "images, labels = generateDatasetArrays(train_dataset)\n",
    "images_test, labels_test = generateDatasetArrays(test_dataset)\n",
    "\n",
    "history = model.fit(\n",
    "    images, labels,\n",
    "    validation_data=(images_test, labels_test),\n",
    "    epochs=Config.EPOCHS,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    verbose=1 if Config.DEBUG else 0\n",
    ")\n",
    "\n",
    "# Feinabstimmung des gesamten Modells\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True  # Alle Schichten freigeben\n",
    "\n",
    "# Reduzierte Lernrate für Fine-Tuning\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"Finetuning des gesamten Modells\")\n",
    "history_finetune = model.fit(\n",
    "    images, labels,\n",
    "    validation_data=(images_test, labels_test),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    verbose=1 if Config.DEBUG else 0\n",
    ")\n",
    "\n",
    "# Kombinieren der Trainingsergebnisse, damit das Trainieren der letzten Ebenenen, sowie das Feintuning in einem Graph visualisiert wird\n",
    "for key in history.history.keys():\n",
    "    history.history[key].extend(history_finetune.history[key])\n",
    "\n",
    "visualize_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen für Testdaten\n",
    "y_pred_new = new_model.predict(X_test_new)\n",
    "\n",
    "# Konvertiere die Vorhersagen zu Klassenindizes\n",
    "y_pred_classes_new = np.argmax(y_pred_new, axis=1)\n",
    "y_test_int_new = np.argmax(y_test_new, axis=1)\n",
    "\n",
    "# Konfusionsmatrix\n",
    "conf_matrix_new = confusion_matrix(y_test_int_new, y_pred_classes_new)\n",
    "disp_new = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_new, display_labels=le_new.classes_)\n",
    "disp_new.plot(cmap=plt.cm.Blues, xticks_rotation=90)\n",
    "plt.title(\"Konfusionsmatrix - Transfer Learning\")\n",
    "\n",
    "#y_pred_classes_new = to_categorical(y_pred_classes_new)\n",
    "\n",
    "#y_pred_classes_new = to_categorical(y_pred_classes_new)\n",
    "report = classification_report(y_test_int_new, y_pred_classes_new, target_names=le_new.classes_)\n",
    "\n",
    "print(report)\n",
    "\n",
    "\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

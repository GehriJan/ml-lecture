{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 11:25:16.641217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733567116.651771   13060 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733567116.654867   13060 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 11:25:16.666449: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 11:25:18.275042: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-12-07 11:25:18.275061: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:137] retrieving CUDA diagnostic information for host: jakob-linux\n",
      "2024-12-07 11:25:18.275067: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:144] hostname: jakob-linux\n",
      "2024-12-07 11:25:18.275116: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:168] libcuda reported version is: 550.135.0\n",
      "2024-12-07 11:25:18.275130: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:172] kernel reported version is: 550.135.0\n",
      "2024-12-07 11:25:18.275134: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:259] kernel version seems to match DSO: 550.135.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_dir = Path().resolve()\n",
    "sys.path.append(str(notebook_dir.parent))\n",
    "\n",
    "from setup import setup_dataset\n",
    "\n",
    "\n",
    "train_dataset, test_dataset, metadata = setup_dataset('../dataset')\n",
    "\n",
    "train_data_dir = '../dataset/train/images'\n",
    "test_data_dir = '../dataset/test/images'\n",
    "verification_image_dir = '../dataset/verification'\n",
    "model_filename = 'dog_classifier.h5'\n",
    "weights_filename = 'model.weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/git/ml-lecture/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-12-07 11:25:22.000371: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/94\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 507ms/step - accuracy: 0.1563 - loss: 52.4929"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "num_classes = 5\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # Number of classes\n",
    "])\n",
    "\n",
    "def preprocess(img, label):\n",
    "    img = tf.image.resize(img, (224, 224))  # Größe anpassen\n",
    "    img = img / 255.0  # Normalisierung\n",
    "    return img, tf.one_hot(label, num_classes)  # One-Hot-Encoding der Labels\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_dataset\n",
    "        .map(preprocess)\n",
    "        .shuffle(1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    test_dataset\n",
    "        .map(preprocess)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save_weights(weights_filename)\n",
    "model.save(model_filename) \n",
    "\n",
    "# Plot Accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performancetest des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Class labels: {0: 'Chihuahua', 1: 'Maltese Dog', 2: 'Komondor', 3: 'German Shepherd', 4: 'African Hunting Dog'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/verification'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Step 5: Test the Model with Example Images\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Update this with the path to your test image directory\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m test_images \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(verification_image_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverification_image_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test images.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m test_images:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/verification'"
     ]
    }
   ],
   "source": [
    "model = load_model(model_filename)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Step 2: Define Class Labels\n",
    "# Ensure these match the class labels used during training\n",
    "class_labels = {\n",
    "    0: \"Chihuahua\",\n",
    "    1: \"Maltese Dog\",\n",
    "    2: \"Komondor\",\n",
    "    3: \"German Shepherd\",\n",
    "    4: \"African Hunting Dog\"\n",
    "}\n",
    "print(\"Class labels:\", class_labels)\n",
    "\n",
    "# Step 3: Define Image Preprocessing Function\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess an image for prediction.\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        target_size (tuple): Desired image size (height, width).\n",
    "    Returns:\n",
    "        np.array: Preprocessed image ready for prediction.\n",
    "    \"\"\"\n",
    "    img = load_img(image_path, target_size=target_size)  # Load and resize the image\n",
    "    img_array = img_to_array(img)  # Convert image to numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return img_array\n",
    "\n",
    "# Step 4: Predict Function\n",
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    Predict the class of an image.\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "    \"\"\"\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class = np.argmax(predictions, axis=-1)[0]\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "\n",
    "    # Display the image with its prediction\n",
    "    img = load_img(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicted: {predicted_label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "# Step 5: Test the Model with Example Images\n",
    "# Update this with the path to your test image directory\n",
    "\n",
    "test_images = [os.path.join(verification_image_dir, f) for f in os.listdir(verification_image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "print(f\"Found {len(test_images)} test images.\")\n",
    "for image_path in test_images:\n",
    "    print(f\"Testing image: {image_path}\")\n",
    "    predict_image(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
